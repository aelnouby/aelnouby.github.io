<section id="projects">
<div class="container">
  <h3>Projects</h3>
  <div class="panel panel-default">
    <div class="panel-body">

      <h5>
        <i class="fa fa-globe"></i>&nbsp;&nbsp;<strong><a href="#">"Real-Time End-to-End Action Detection with Two-Stream Networks", submitted to CRV </a></strong></h5>
        <p>

        </p>
        <ul>
          <li>Alaaeldin El-Nouby, Graham W. Taylor</li>
          <li><a href="https://arxiv.org/submit/2172908/view">Arxiv</a></li>
        </ul>
        <div align="center">
            <figure><img width=448 height="320" src="static/img/crv.gif"></figure>
        </div>

      <h5>
  			<i class="fa fa-globe"></i>&nbsp;&nbsp;<strong><a href="#">Text to Image Syntehsis | +50 stars </a></strong></h5>
        <p>
          A Pytroch implementation of the Generative Adversarial Text to Image Synthesis <a href='https://arxiv.org/abs/1605.05396'>paper</a>. In addition to implementing the proposed architecture in the paper, I have implemented few techniques (feature matching, minibatch-discrimnation, WGAN) for more stable training and higher generation quality.
        </p>
        <ul>
          <li><a href="https://www.researchgate.net/publication/322211987_Improved_Text_to_image_synthesis_using_Generative_Adversarial_Networks">PDF</a></li>
          <li><a href="https://github.com/aelnouby/Text-to-Image-Synthesis"> Code </a></li>
        </ul>
        <div align="center">
            <figure><img width=300 height="300" src="https://github.com/aelnouby/Text-to-Image-Synthesis/blob/master/images/64_flowers.jpeg?raw=true"></figure>
        </div>


		<h5>
			<i class="fa fa-globe"></i>&nbsp;&nbsp;<strong><a href="#">Bachelor's Thesis</a></strong></h5>
      <p>
      Many large cities have crime and antisocial behavior problems, such as fights, vandalism, breaking and entering shop windows, etc. Often these cities have video cameras already installed. We propose a smart surveillance system to detect abandoned luggage in airports and public transportations, provide continuous analysis of people behaviours, discover potential threats in time and eventually prevent dangerous situations.
      </p>
      <ul>
        <li><a href="https://drive.google.com/file/d/0B4mutBv2Nvu3RjVxYUJPU2k3d2M/view?usp=sharing">Thesis</a></li>
        <li><a href="https://github.com/Grad-CSE2016"> Code </a></li>
      </ul>
      <div align="center">
        <iframe width="450" height="280"
          src="https://www.youtube.com/embed/xj3l7neOS0Q">
        </iframe>
      </div>


		<h5>
			<i class="fa fa-github"></i>&nbsp;&nbsp;<strong><a href="#">Cross Data-set Action Detection</a></strong></h5>
      <p>
      I have worked on the implementation of the “Cross dataset action detection” <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/05539875-publish.pdf">paper</a> by <a href="http://www.ifp.illinois.edu/~cao4/">Liangliang Cao</a>. It builds a framework using GMMs that achieves two very complex tasks, first it adapts the GMMs trained on a source dataset which is fully annotated, to a target dataset with minimum or even no annotation at all; Secondly, it detects and localizes the regions where the action is happening.
      </p>
      <ul>
        <li><a href="https://github.com/aelnouby/Cross-Dataset-Action-Detection">Code</a></li>
      </ul>
      <div align="center">
        <figure>
          <img width="400" height="150" src="static/img/GMM.png">
          <figcaption>Using of foreground and background scores to localize the action</figcaption>
        </figure>
      </div>

		<h5>
			<i class="fa fa-github"></i>&nbsp;&nbsp;<strong><a href="#">Action Recognition</a></strong></h5>
      <p>
      I have implemented a pipeline for action recognition that classifies actions using interest points in 3D which are then quantized using K-means and represented in a Bag of words then classified using non-linear SVM.
      </p>
      <ul>
        <li><a href="https://github.com/Grad-CSE2016/Action-Recognition">Code</a></li>
      </ul>

      <h5>
        <i class="fa fa-cog"></i>&nbsp;&nbsp;<strong><a href="#">Face Recognition</a></strong></h5>
        <p>In my Internship at <a href="http://www.avidbeam.com/">AvidBeam</a>, I worked on implementing facial Keypoints extraction for face recognition.</p>
        <ul>
          <li><a href="https://drive.google.com/file/d/0B-3EpvYAHo2OY2ZlaEZFaFhjM28/view?usp=sharing">Presentation</a></li>
        </ul>

		<h5>
			<i class="fa fa-cog"></i>&nbsp;&nbsp;<strong><a href="https://docs.google.com/document/d/1CSmPtAzq1s2NV4r60VaZNm7wCgpHunOeev2wx-kAQIM/edit?usp=sharing">GSOC-16 Pgmpy Proposal</a></strong></h5>
			Tabular representation of factors is very inefficient for large networks, ADDs are a much more efficient alternative.
    <h5>
      <i class="fa fa-globe"></i>&nbsp;&nbsp;<strong><a href="https://github.com/aelnouby/Udacity-Deep-Learning-Assignments">Udacity Deep Learning Course Assignments</a></strong>
    </h5>

    </div>
  </div>
</div>
</section>
